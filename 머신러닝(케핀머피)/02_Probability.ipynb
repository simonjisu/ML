{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Introduction\n",
    "\n",
    "두 가지 관점:\n",
    "\n",
    "1. frequenist interpretation: probabilities represent long run frequencies of events\n",
    "2. bayesian interpretation: probability is used to quantify our uncertainty about something\n",
    "\n",
    "베이지안 확률 정의의 장점:  can be used to model our uncertainty about events that do not have long term frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 A brief review of probability theory\n",
    "\n",
    "### 2.2.1 Discrete random variables\n",
    "\n",
    "$p(A)$: the probability that the event $A$ is true\n",
    "$p(\\bar{A}) = 1-p(A)$: the probability that the event not $A$\n",
    "\n",
    "**require:**\n",
    "\n",
    "* $0 \\leq p(A) \\leq 1$: \n",
    "    * 0: the event is definitely not happen\n",
    "    * 1: the event is definitely happen\n",
    "\n",
    "the notion of binary events by defining a **discrete random variable** $X$, which can take on any value from a finite or countably infinite set $\\varkappa$. Denote the probability of the event that $X=x$ by $p(X=x)$, for short is $p(x)$. $p()$ is called **probability mass function(pmf)**. It satisfies $0 \\leq p(x) \\leq 1$ and $\\sum_{x \\in \\varkappa} p(x) = 1$\n",
    "\n",
    "* $\\varkappa$: state space\n",
    "\n",
    "Uniform distribution:\n",
    "\n",
    "$\\Bbb{I}()$ is the binary **indicator function**. \n",
    "\n",
    "\n",
    "### 2.2.2 Fundamental Rules\n",
    "\n",
    "#### Probability of a union of two events\n",
    "$$\\begin{aligned}p(A\\cup B) &= p(A) + p(B) +p(A\\cap B)\\\\ \n",
    "&= p(A) + p(B) \\quad if\\ A\\ and\\ B\\ are\\ mutually\\ exclusive \\end{aligned}$$\n",
    "\n",
    "####  Joint probabilities\n",
    "$$p(A,B) = p(A\\cap B) = p(A \\vert B)p(B)$$\n",
    "\n",
    "the marginal distribution: \n",
    "\n",
    "$$p(A) = \\sum_b p(A, B) = \\sum_b p(A \\vert B=b)p(B=b)$$\n",
    "\n",
    "the sum rule / the rule of total probability:\n",
    "\n",
    "$$p(X_{1:D}) = p(X_1)p(X_2 \\vert X_1)p(X_3 \\vert X_2, X_1) \\cdots p(X_D \\vert X_{1:D-1})$$\n",
    "\n",
    "#### Conditional probability\n",
    "$$p(A \\vert B) = \\dfrac{p(A, B)}{p(B)},\\ if\\ p(B) > 0$$\n",
    "\n",
    "### 2.2.3  Bayes rule(Bayes Theorem)\n",
    "$$p(X=x \\vert Y=y) = \\dfrac{p(X=x, Y=y)}{p(Y=y)} = \\dfrac{p(Y=y \\vert X=x)p(X=x)}{\\sum_{x'}p(Y=y \\vert X=x')p(X=x')}$$\n",
    "\n",
    "Example: medical diagnosis\n",
    "\n",
    "> mammogram: a medical test for breast cancer for a woman in 40s \n",
    ">\n",
    "> the test has a sensitivity of 80%, which means, if you have cancer, the test will be positive with probability 0.8\n",
    ">\n",
    "> $$p(x=1 \\vert y=1)=0.8$$\n",
    ">\n",
    "> * $x = 1$ is the event the mammogram is positive, $y = 1$ is the event you have breast cancer\n",
    ">\n",
    "> the perior probability(base rate fallacy) of having breast cancer is $p(y=1) = 0.004$\n",
    ">\n",
    "> a false positive / false alarm\n",
    ">\n",
    "> $$p(x=1 \\vert y=0)=0.1$$\n",
    ">\n",
    "> We want to calculate wheter the probability having breast cancer when we take the test, Using bayes rule\n",
    ">\n",
    "> $$\\begin{aligned} p(y=1 \\vert x=1) &= \\dfrac{p(x=1 \\vert y=1)p(y=1)}{p(x=1 \\vert y=1)p(y=1) + p(x=1 \\vert y=0)p(y=0)} \\\\\n",
    "&= \\dfrac{0.8 \\times 0.004}{0.8\\times 0.004 + 0.1 \\times 0.996} \\\\ &= 0.031\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0311284046692607\n"
     ]
    }
   ],
   "source": [
    "p_y1 = 0.004\n",
    "p_x1y1 = 0.8\n",
    "p_x1y0 = 0.1\n",
    "p_y1x1 = (p_x1y1 * p_y1) / (p_x1y1*p_y1 + p_x1y0*(1-p_y1))\n",
    "print(p_y1x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Generative classifiers:\n",
    "\n",
    "$$p(y=c \\vert x, \\theta) = \\dfrac{p(x \\vert y=c, \\theta)p(y=c \\vert \\theta)}{\\sum_{c'}p(x \\vert y=c', \\theta)p(y=c' \\vert \\theta)}$$\n",
    "\n",
    "### 2.2.4 Independence and conditional independence\n",
    "\n",
    "$X\\perp Y \\Leftrightarrow p(X, Y)=p(X)p(Y)$: unconditionally independent / marginally independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/02_independent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X, Y$ 가 각각 descrete random variable 이라고 하자, 예를 들어 $X$ 는 6면체 주사위, $Y$ 는 5면체의 주사위라고 생각하면, X, Y 들 던졌을 때의 경우의 수를 생각하면 총 필요한 파라미터가 29 (왜 -1?), 독립일 때는 (6-1) + (5-1) = 9 개의 파라미터가 필요함\n",
    "\n",
    "1/30 ~ 1, 1/6~1 + 1/5~1 ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ and $Y$ are conditionally independent (CI) given $Z$ iff the conditional joint can be written as a product of conditional marginals:\n",
    "\n",
    "$$X\\perp Y \\vert Z \\Leftrightarrow p(X, Y \\vert Z)= p(X \\vert Z)p(Y \\vert Z)$$\n",
    "\n",
    "#### Theorm 2.2.1\n",
    "$X\\perp Y \\vert Z$ iff there exist function $g$ and $h$ such that, $p(x, y \\vert z)= g(x, z)h(y, z)$ for all $x,y,z$ such that $p(z)>0$\n",
    "\n",
    "### 2.2.5 Continuous random variables\n",
    "\n",
    "Suppose $X$ is some uncertain continuous quantity. The probability that $X$ lies in any interval $a \\leq X \\leq b$ can be computed as follows. Define the events $A=(X\\leq a), B=(X \\leq b), W=(a < X \\leq b)$. \n",
    "\n",
    "We have that $B = A \\cup W$ and since $A$ and $W$ are mutually exclusive, the sum rules gives $p(B) = p(A) + p(W)$, hence $p(W) = p(B) - p(A)$\n",
    "\n",
    "Define the function $F(q) \\triangleq p(X \\leq q)$ called as **cumulative distribution function(cdf)** of $X$. This is obviously a monotonically increasing function. Using this notation we have\n",
    "\n",
    "$$p(a < X \\leq b) = F(b) - F(a)$$\n",
    "\n",
    "Now define, $f(x) = \\frac{d}{dx}F(x)$ (we assume this derivative exists); this is called the **probability\n",
    "density function(pdf)**. Given a pdf, we can compute the probability of a continuous variable being in a finite interval as follows:\n",
    "\n",
    "$$P(a < X \\leq b)=\\int_a^b f(x) dx$$\n",
    "\n",
    "As the size of the interval gets smaller, we can write\n",
    "\n",
    "$$P(x \\leq X \\leq x + dx) \\approx p(x) dx$$\n",
    "\n",
    "* require $p(x) \\geq 0$\n",
    "* but it is possible for $p(x) > 1$ for any given $x$\n",
    "    * consider the **uniform distribution** Unif(a,b): $Unif(x \\vert a, b) = \\dfrac{1}{b-a} \\Bbb{I}(a \\leq x \\leq b)$\n",
    "    * if we set $a=0, b=\\frac{1}{2}$ we have $p(x)=2$ for any $x \\in [0, \\frac{1}{2}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Quantiles\n",
    "\n",
    "Since the cdf $F$ is a monotonically increasing function, it has an inverse; let us denote this by $F^{-1}$, If $F$ is the cdf of $X$, then $F^{-1}(\\alpha)$ is the value of $x_{\\alpha}$ such that $P(X \\leq x_{\\alpha}) = \\alpha$, this is called the $\\alpha$ **quantile** of $F$.\n",
    "\n",
    "We can also use the inverse cdf to compute **tail area probabilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.7 Mean and variance\n",
    "\n",
    "**mean / expected value** ($\\mu$) \n",
    "\n",
    "$$\\begin{cases} \\Bbb{E}[X] \\triangleq \\sum_{x\\in \\varkappa} x p(x) \\quad for\\ descrete\\ rv's \\\\\n",
    "\\Bbb{E}[X] \\triangleq \\int_{\\varkappa} x p(x) dx \\quad for\\ continuous\\ rv's \n",
    "\\end{cases}$$\n",
    "\n",
    "If this integral is not finite, the mean is not defined \n",
    "\n",
    "The **variance**(\\sigma^2) is a measure of the “spread” of a distribution\n",
    "\n",
    "$$\\begin{aligned} var[X] &\\triangleq \\Bbb{E}[(X - \\mu)^2] = \\int (x-\\mu)^2 p(x) dx \\\\\n",
    "&= \\int x^2 p(x) dx + \\mu^2 \\int p(x) dx - 2\\mu \\int x p(x) dx = \\Bbb{E}[X^2] - \\mu^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "* so, $\\Bbb{E}[X^2] = \\mu^2 + \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Some Common discrete distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3.1 The binomial and Bernoulli distributions\n",
    "\n",
    "$$X \\sim Bin(n, \\theta)$$\n",
    "\n",
    "pmf:\n",
    "\n",
    "$$Bin(k \\vert n, \\theta) \\triangleq \\dbinom{k}{n} \\theta^{k} (1-\\theta)^{n-k}$$\n",
    "\n",
    "* $\\dbinom{k}{n} = \\dfrac{n!}{(n-k)!k!}$: binomial coefficient\n",
    "\n",
    "When $n=1$, it becomes Bernoulli distribution.\n",
    "\n",
    "$$Bern(x \\vert \\theta) = \\theta^{\\Bbb{I}(x=1)}(1-\\theta)^{\\Bbb{I}(x=0)}$$\n",
    "\n",
    "### 2.3.2 The multinomial and multinoulli distributions\n",
    "\n",
    "pmf:\n",
    "\n",
    "$$Mu(x \\vert n, \\theta) \\triangleq \\begin{pmatrix} n \\\\ x_1 \\cdots x_K \\end{pmatrix} \\prod_{j=1}^K \\theta_j^{x_j}$$\n",
    "\n",
    "* $x = (x_1, \\cdots, x_K)$: random vector, where $x_j$ is number of j-th class data appeared\n",
    "* $\\begin{pmatrix} n \\\\ x_1 \\cdots x_K \\end{pmatrix} = \\dfrac{n!}{x_1!\\cdots x_K!}$\n",
    "\n",
    "When $n = 1$, we can use **dummy encoding(one hot encoding)**, $x$ becomes $x = [\\Bbb{I}(x = 1), \\cdots ,\\Bbb{I}(x = K)]$, and pmf becomes:\n",
    "\n",
    "$$Mu(x \\vert 1, \\theta)=\\prod_{j=1}^K \\theta_j^{\\Bbb{I}(x_j=1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/02_summary_binomial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 The Poisson distribution\n",
    "\n",
    "$$X \\sim Poi(\\lambda), \\quad \\lambda >0, X \\in \\{0, 1, 2 \\cdots\\}$$\n",
    "\n",
    "pmf:\n",
    "\n",
    "$$Poi(x \\vert \\lambda) = e^{-\\lambda} \\dfrac{\\lambda^x}{x!}$$\n",
    "\n",
    "* $e^{-\\lambda}$ : the normalization constant, required to ensure the distribution sums to 1\n",
    "\n",
    "The Poisson distribution is often used as a model for counts of rare events like radioactive decay and traffic accidents.\n",
    "\n",
    "### 2.3.4  The empirical distribution\n",
    "\n",
    "Given set of data, $D=\\{x_1, \\cdots x_N \\}$, define **empirical distribution(measure)** as follow:\n",
    "\n",
    "$$p_{emp}(A) \\triangleq \\dfrac{1}{N} \\sum_{i=1}^N \\delta_{x_i}(A)$$\n",
    "\n",
    "* $\\delta_{x_i}(A) = \\begin{cases} 0 \\quad if x \\notin A\\\\ 1 \\quad if x \\in A\\end{cases}$: Dirac measure\n",
    "\n",
    "In general, we can associate \"weights\" with each sample:\n",
    "\n",
    "$$p(x) = \\sum_{i=1}^N w_i \\delta_{x_i}(x)$$\n",
    "\n",
    "where require $0 \\leq w_i \\leq 1$ and $\\sum_{i=1}^N w_i = 1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Some common continuous distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Gaussian (normal) distribution\n",
    "\n",
    "$$X \\sim N(\\mu, \\sigma^2)$$\n",
    "\n",
    "pdf:\n",
    "\n",
    "$$N(x \\vert \\mu, \\sigma^2) \\triangleq \\dfrac{1}{\\sqrt{2\\pi \\sigma^2} } e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2 }$$\n",
    "\n",
    "* $\\mu = \\Bbb{E}[X]$\n",
    "* $\\sigma^2 = var[X]$\n",
    "* $\\sqrt{2\\pi \\sigma^2}$: normalization constant needed to ensure the density integrates to 1\n",
    "\n",
    "When $\\mu=0, \\sigma^2 = 1$, then $X$ follows a **standard normal distribution**\n",
    "\n",
    "**precision** of a Gaussian: $\\lambda = \\dfrac{1}{\\sigma^2}$\n",
    "\n",
    "cdf:\n",
    "\n",
    "$$\\Phi(x; \\mu, \\sigma^2) \\triangleq \\int_{-\\infty}^x N(z \\vert \\mu, \\sigma^2) dz$$\n",
    "\n",
    "The Gaussian distribution is the most widely used distribution in statistics. There are several reasons for this. \n",
    "\n",
    "1. First, it has two parameters which are easy to interpret, and which capture some of the most basic properties of a distribution, namely its mean and variance. \n",
    "2. Second, the central limit theorem tells us that sums of independent random variables have an approximately Gaussian distribution, making it a good choice for modeling residual errors or “noise”. \n",
    "3. Third, the Gaussian distribution makes the least number of assumptions (has maximum entropy), subject to the constraint of having a specified mean and variance, as we show in Section 9.2.6; this makes it a good default choice in many cases. \n",
    "4. Finally, it has a simple mathematical form, which results in easy to implement, but often highly effective, methods, as we will see. \n",
    "\n",
    "### 2.4.2 Degenerate pdf\n",
    "\n",
    "limit $\\sigma^2 \\rightarrow 0$, gaussian dist becomes \n",
    "\n",
    "$$\\underset{\\sigma^2 \\rightarrow 0}{\\lim} N(x \\vert \\mu, \\sigma^2) = \\delta(x-\\mu)$$\n",
    "\n",
    "*  Dirac delta function: $\\delta(x) = \\begin{cases} \\infty \\quad if\\ x = 0 \\\\ 0 \\quad if\\ x \\not = 0\\end{cases}$, such that $\\int_{-\\infty}^{\\infty} \\delta(x)dx = 1$\n",
    "\n",
    "A useful property of delta functions is the sifting property, which selects out a single term from a sum or integral:\n",
    "\n",
    "$\\int_{-\\infty}^{\\infty} f(x)\\delta(x-\\mu) dx = f(\\mu)$\n",
    "\n",
    "since the integrand is only non-zero if $x-\\mu = 0$\n",
    "\n",
    "One problem with the Gaussian distribution is that it is sensitive to outliers, since the logprobability only decays quadratically with distance from the center. A more robust distribution is the **Student t distribution**\n",
    "\n",
    "$$T(x \\vert \\mu, \\sigma^2, v) \\propto \\lbrack 1 + \\dfrac{1}{v}(\\dfrac{x-\\mu}{\\sigma})^2 \\rbrack^{-\\frac{v+1}{2}}$$\n",
    "\n",
    "* $v>0$: degrees of freedom\n",
    "* mean = mode = $\\mu$, variance = $\\dfrac{v\\sigma^2}{(v-2)}$\n",
    "* The variance is only defined if $v > 2$. The mean is only defined if $v > 1$\n",
    "* robustness at lower $v$, because it has fat tail than gaussian.\n",
    "\n",
    "If $v = 1$, this distribution is known as the **Cauchy or Lorentz** distribution. This is notable for having such heavy tails that the integral that defines the mean does not converge.\n",
    "\n",
    "For $v \\gg 5$, the Student distribution rapidly approaches a Gaussian distribution and loses its robustness properties.\n",
    "\n",
    "### 2.4.3 The Laplace distribution\n",
    "\n",
    "Another distribution with heavy tails is the **Laplace distribution(double sided exponential distribution)**\n",
    "\n",
    "pdf:\n",
    "\n",
    "$$Lap(x \\vert \\mu, b) \\triangleq \\dfrac{1}{2b} \\exp(-\\dfrac{\\vert x-\\mu \\vert}{b})$$\n",
    "\n",
    "* $\\mu$: location parameter\n",
    "* $b > 0$: scale parameter.\n",
    "* mean = mode = $\\mu$, variance = $2b^2$\n",
    "* put mores probability density at 0 than the Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/02_robustness.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 The gamma distribution\n",
    "\n",
    "a flexible distribution for positive real valued rv’s, $x > 0$.\n",
    "\n",
    "$$Ga(T \\vert shape=a, rate=b) \\triangleq \\dfrac{b^a}{\\Gamma(a)}T^{a-1}e^{-Tb}$$\n",
    "\n",
    "* where $\\Gamma(a) \\triangleq \\int_0^{\\infty} u^{x-1}e^{-u}du$\n",
    "* shape: $a > 0$ \n",
    "* rate: $b > 0$\n",
    "* mean = $\\dfrac{a}{b}$, mode = $\\dfrac{a-1}{b}$, var = $\\dfrac{a}{b^2}$\n",
    "\n",
    "Speacial cases:\n",
    "\n",
    "* **Exponential distribution**: $Expon(x \\vert \\lambda) \\triangleq Ga(x \\vert 1, \\lambda)$ This distribution describes the times between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate $\\lambda$\n",
    "* **Erlang distribution**: common $a=2 Ga(x \\vert shape=2, rate=\\lambda)$\n",
    "* **Chi-squared distribution**: $\\chi^2(x \\vert v) \\triangleq Ga(x \\vert \\frac{v}{2}, \\frac{1}{2})$ This is the distribution of the sum of squared Gaussian random variables. $Z_i \\sim N(0,1)$ and $S=\\sum_{i=1}^v Z_i^2$ then $S \\sim \\chi_v^2$\n",
    "\n",
    "**inverse gamma** distribution:\n",
    "\n",
    "$$\\dfrac{1}{X} \\sim IG(a, b)$$\n",
    "\n",
    "$$IG(x \\vert shape=a, rate=b) \\triangleq \\dfrac{b^a}{\\Gamma(a)}x^{-(a+1)}e^{-b/x}$$\n",
    "\n",
    "* mean = $\\dfrac{b}{a-1}$, mode = $\\dfrac{b}{a+1}$, var = $\\dfrac{b^2}{(a-1)^2(a-2)}$\n",
    "* The mean only exists if $a > 1$. The variance only exists if $a > 2$.\n",
    "\n",
    "### 2.4.5 The beta distribution\n",
    "\n",
    "**beta distribution** has support over the interval $[0, 1]$\n",
    "\n",
    "$$Beta(x \\vert a, b) = \\dfrac{1}{B(a,b)} x^{a-1}(1-x)^{b-1}$$\n",
    "\n",
    "* $B(a,b) \\triangleq \\dfrac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$\n",
    "* $a, b > 0$ to ensure the distribution is integrable \n",
    "* mean = $\\dfrac{a}{a+b}$, mode = $\\dfrac{a-1}{a+b-2}$, var = $\\dfrac{ab}{(a+b)^2(a+b+1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/02_beta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.6 Pareto distribution\n",
    "\n",
    "it used to model the distribution of quantities that exhibit **long tails(heavy tails)**.\n",
    "\n",
    "example: Zipf’s law\n",
    "\n",
    "pdf:\n",
    "\n",
    "$$Pareto(x \\vert k, m)=km^kx^{-(k+1)}\\Bbb{I}(x \\geq m)$$\n",
    "\n",
    "* This density asserts that $x$ must be greater than some constant $m$, but not too much greater, where $k$ controls what is “too much”.\n",
    "* As $k \\rightarrow \\infty$, the distribution approaches $\\delta(x − m)$\n",
    "\n",
    "log scale:(power law)\n",
    "\n",
    "$$\\log p(x) = a \\log x + c$$\n",
    "\n",
    "* mean = $\\dfrac{km}{k-1}, \\quad k>1$, mode = $m$, var = $\\dfrac{m^2k}{(k-1)^2(k-2)}, \\quad if\\ k>2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Joint probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.5.1 Covariance and correlation\n",
    "\n",
    "The **covariance** between two rv’s $X$ and $Y$ measures the degree to which $X$ and $Y$ are (linearly) related.\n",
    "\n",
    "$$cov[X, Y] \\triangleq \\Bbb{E}[(X - \\Bbb{E}[X])(Y - \\Bbb{E}[Y])] = \\Bbb{E}[XY] - \\Bbb{E}[X]\\Bbb{E}[Y]$$\n",
    "\n",
    "If $x$ is a $d$-dimensional random vector, its **covariance matrix** is defined to be the following symmetric, positive definite matrix:\n",
    "\n",
    "$$\\begin{aligned} cov[x] \n",
    "&\\triangleq \\Bbb{E}[(x - \\Bbb{E}[x])(x - \\Bbb{E}[x])^T] \\\\\n",
    "&= \\begin{pmatrix} \n",
    "var[x_1] & cov[x_1, x_2] & \\cdots & cov[x_1, x_d] \\\\\n",
    "cov[x_2, x_1] & var[x_2] & \\cdots & cov[x_2, x_d] \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "cov[x_d, x_1] & cov[x_d, x_2] & \\cdots & var[x_d]\n",
    "\\end{pmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "**(Pearson) correlation coefficient**:\n",
    "\n",
    "$$corr[X, Y] = \\dfrac{cov[X, Y]}{\\sqrt{var[X]var[Y]} }$$\n",
    "\n",
    "correlation matrix:\n",
    "\n",
    "$$\\begin{pmatrix} \n",
    "corr[x_1, x_1] & corr[x_1, x_2] & \\cdots & corr[x_1, x_d] \\\\\n",
    "corr[x_2, x_1] & corr[x_2, x_2] & \\cdots & corr[x_2, x_d] \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "corr[x_d, x_1] & corr[x_d, x_2] & \\cdots & corr[x_d, x_d]\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "* $-1 \\leq corr[X, Y] \\leq 1$\n",
    "\n",
    "### 2.5.2 The multivariate Gaussian\n",
    "\n",
    "$$N(x \\vert \\mu, \\sum) \\triangleq \\dfrac{1}{(2\\pi)^{D/2}\\vert D \\vert^{1/2} } \\exp [-\\dfrac{1}{2}(x-\\mu)^T \\sum^{-1} (x-\\mu)]$$\n",
    "\n",
    "* mean vector: $\\mu = \\Bbb{E}[x] \\in \\Bbb{R}^D$ \n",
    "* covariance matrix: $\\sum = cov[x]$\n",
    "* covariance matrix has $D(D + 1)/2$ parameters, divide by 2 since $\\sum$ symmetric\n",
    "\n",
    "**precision matrix (concentration matrix)**:\n",
    "\n",
    "$\\Lambda = \\sum^{-1}$\n",
    "\n",
    "* $(2\\pi)^{-D/2}\\vert \\Lambda \\vert^{1/2}$ ensures that the pdf integrates to 1.\n",
    "\n",
    "### 2.5.3 Multivariate Student t distribution\n",
    "\n",
    "$$\\begin{aligned} T(x \\vert \\mu, \\sigma, v) &= \\dfrac{\\Gamma(v/2 + D/2)}{\\Gamma(v/2)} \\dfrac{\\vert \\sum \\vert^{-1/2} }{v^{D/2} \\pi^{D/2} } \\times [1 + \\dfrac{1}{v} (x-\\mu)^T \\sum^{-1} (x-\\mu)]^{-(\\frac{v+D}{2}) } \\\\\n",
    "&= \\dfrac{\\Gamma(v/2 + D/2)}{\\Gamma(v/2)} \\vert \\pi V \\vert^{-1/2} \\times [1 + (x-\\mu)^T V^{-1} (x-\\mu)]^{-(\\frac{v+D}{2}) }\n",
    "\\end{aligned}$$\n",
    "\n",
    "* $\\sum$: scale matrix\n",
    "* $V = v\\sum$\n",
    "* This has fatter tails than a Gaussian. The smaller $v$ is, the fatter the tails.\n",
    "* mean = mode = $\\mu$, Cov = $\\dfrac{v}{v-2}\\sum$ \n",
    "\n",
    "### 2.5.4 Dirichlet distribution\n",
    "\n",
    "A multivariate generalization of the beta distribution is the **Dirichlet distribution**, has support over the **probability simplex**\n",
    "\n",
    "$$S_k = \\{x: 0 \\leq x_k \\leq 1, \\sum_{k=1}^K x_k=1 \\}$$\n",
    "\n",
    "pdf:\n",
    "\n",
    "$$Dir(x \\vert \\alpha) \\triangleq \\dfrac{1}{B(\\alpha)} \\prod_{k=1}^K x_k^{\\alpha_k -1} \\Bbb{I}(x \\in S_K)$$\n",
    "\n",
    "* $B(\\alpha_1, \\cdots , \\alpha_K) \\triangleq \\dfrac{\\prod_{k=1}^K \\Gamma(\\alpha_k)}{\\Gamma(\\alpha_0)}$ where $\\alpha_0 \\triangleq \\sum_{k=1}^K \\alpha_k$: the natural generalization of the beta function to $K$ variables\n",
    "* $\\Bbb{E}[x_k] = \\dfrac{\\alpha_k}{\\alpha_0}$, $mode[x_k] = \\dfrac{\\alpha_k -1}{\\alpha_0 - K}$, $var[x_k] = \\dfrac{\\alpha_k(\\alpha_0-\\alpha_k)}{\\alpha_0^2(\\alpha_0+1)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Transformations of random variables\n",
    "If $x \\sim p()$ is some random variable, and $y = f(x)$, what is the distribution of $y$? This is the question we address in this section.\n",
    "\n",
    "### 2.6.1 Linear transformations\n",
    "\n",
    "$y = f(x) = Ax + b$\n",
    "\n",
    "* $\\Bbb{E}[y] = A\\mu + b$ \n",
    "* $cov[y] = A \\sum A^T$\n",
    "\n",
    "### 2.6.2 General transformations\n",
    "\n",
    "If $X$ is a discrete rv, we can derive the pmf for $y$ by simply summing up the probability mass for all the $x$’s such that $f(x) = y$:\n",
    "\n",
    "$$p_y(y) = \\underset{x:f(x)=y}{\\sum} p_x(x)$$\n",
    "\n",
    "If X is continuous rv, cdf: \n",
    "\n",
    "$$P_y(y) \\triangleq P(Y \\leq y) = P(f(X) \\leq y) = P(X \\in \\{x \\vert f(x) \\leq y\\})$$\n",
    "\n",
    "derive pdf of $y$ by differentiating the cdf.\n",
    "\n",
    "In the case of monotonic and hence invertible functions, we can write\n",
    "\n",
    "$$P_y(y) \\triangleq P(Y \\leq y) = P(f(X) \\leq f^{-1}(y)) = P_x(f^{-1}(y))$$\n",
    "\n",
    "$p_y(y) \\triangleq \\dfrac{d}{dy} P_y(y) = P_x(f^{-1}(y)) = \\dfrac{dx}{dy} \\dfrac{d}{dx} P_x(x) = \\dfrac{dx}{dy} p_x(x)$ where $x=f^{-1}(y)$\n",
    "\n",
    "Since the sign of this change is not important, we take the absolute value to get the general expression: **change of variables** formula\n",
    "\n",
    "$$p_y(y) = p_x(x)\\vert \\dfrac{dx}{dy} \\vert$$\n",
    "\n",
    "\n",
    "### 2.6.3 Central limit theorem\n",
    "\n",
    "Now consider $N$ random variables with pdf’s (not necessarily Gaussian) $p(x_i)$, each with mean $\\mu$ and variance $\\sigma^2$. We assume each variable is **independent and identically distributed(iid)**. Let $S_N = \\sum_{i=1}^N X_i$ be the sum of the rv’s. This is a simple but widely used transformation of rv’s. One can show that, as $N$ increases, the distribution of this sum approaches\n",
    "\n",
    "$$p(S_N=s) = \\dfrac{1}{\\sqrt{2\\pi N \\sigma^2} } \\exp(-\\dfrac{(s-N\\mu)^2}{2N \\sigma^2})$$\n",
    "\n",
    "Hence the distribution of the quantity $Z_N \\triangleq \\dfrac{S_N-N_{\\mu} }{\\sigma \\sqrt{N} } = \\dfrac{\\bar{X}-\\mu }{\\sigma / \\sqrt{N} }$ converges to the standard normal, where $\\bar{X} = \\dfrac{1}{N} \\sum_{i=1}^N x_i$ is the sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Monte Carlo approximation\n",
    "One way to compute the distribution of a function of an rv using the change of variables formula: **Monte Carlo approximation**\n",
    "\n",
    "1. generate $S$ samples from the distribution: $x_1, \\cdots x_S$\n",
    "2. Given the samples, we can approximate the distribution of $f(X)$ by using the empirical distribution of $\\{ f(x_s) \\}_{s=1}^S$\n",
    "\n",
    "Monte Carlo integration:\n",
    "\n",
    "$\\Bbb{E}[f(X)] = \\int f(x)p(x)dx \\approx \\dfrac{1}{S} \\sum_{s=1}^S f(x_s)$ where $x_s \\sim p(X)$\n",
    "\n",
    "넘나 어렵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.1 Entropy\n",
    "\n",
    "$\\Bbb{H}(X)$($\\Bbb{H}(p)$) : The entropy of a random variable $X$ with distribution $p$, a measure of its uncertainty\n",
    "\n",
    "In particular, for a discrete variable with $K$ states: \n",
    "\n",
    "$$\\Bbb{H}(X) \\triangleq - \\sum_{k=1}^K p(K=k) log_2p(X=k)$$\n",
    "\n",
    "* base = 2: **bits**, base = $e$: **nats**\n",
    "\n",
    "binary entropy function: $X \\in \\{0, 1\\}$, $p(X=1)=\\theta, p(x=0)=1-\\theta$\n",
    "\n",
    "$$\\begin{aligned}\\Bbb{H} &= -[p(X=1)\\log_2p(X=1)+p(X=0)\\log_2p(X=0)] \\\\\n",
    "&= -[\\theta \\log_2\\theta+(1-\\theta)\\log_2(1-\\theta)]\\end{aligned}$$\n",
    "\n",
    "### 2.8.2 KL divergence\n",
    "\n",
    "Kullback-Leibler divergence (KL divergence, relative entropy): to measure the dissimilarity of two probability distributions, $p$ and $q$\n",
    "\n",
    "$$$$\n",
    "\n",
    "### 2.8.3 Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
